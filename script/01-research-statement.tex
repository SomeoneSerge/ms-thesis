% From Skoltech's requirements:
%
% > The main part includes:
% > - A description of the history and background literature on the subject
% > - Statement of research problem
% > - Statement of methodology
% >     - Design, data collection, analysis, interpretation
% > - Results
% > - Discussion of innovation and research findings
% > 
% > Justification of reseach relevance is mandatory.


\chapter{Premises} \label{chap:premises}

\section{Emergence of hyperbolic deep learning} \label{sec:history}

In 2017-2018~\cite{nickelKiela17} it came to attention of deep learning community, that for a
Euclidean space of fixed dimension, \( \mathbb{R}^n \) with usual distance
function, there are graphs which cannot be isometrically embedded in it, and
that any graph could be isometrically embedded in a Hyperbolic space.
Sarkar~\cite{sarkar} gives a combinatorial algorithm for approximately
isometric embedding of a tree. This has inspired a number of works using
hyperbolic representations for graph-like data, for instance in language models

\subsection*{Curvature in metric graphs}

Consider a graph that consists of vertices connected by edges. Such a graph is
a metric space -- its points are the nodes of the graph, and distances are
measured in smallest number of edges on has to travel to get from one node to
another.  Now, such distances can behave very wildly, compared to Euclidean
space. For instance, you could have any number of points that are
simulataneosuly nearest neighbours of each other (consider the fully-connected
graph \( K_n \)), whereas in a two-dimensional Euclidean
plane~\cite{howManyNeighbours} the best you can do is three points allocated in
an equilateral triangle. This has immediate implications if you were to define
the notion of ``similarity'' of objects (images or words) in terms of distance
between their embeddings.  For instance, we could say that objects are
``similar'' if the ``distance'' between them is no more than \( 1 \) (an
arbitrarily chosen threshold).  That's a binary relation, and with
graph-distances such relation could well be non-transitive. This is easily seen
in a tree of constant branching factor: the sibling nodes are both ``similar''
to their common immediate parent, but not to each other. For a ``branchy''
enough tree, such relationships simply cannot be modeled by distances between
Euclidean embeddings (of pre-defined dimension).

The reason such isometric embeddings into Euclidean space are impossible for
certain graphs is that these graphs have \emph{curvature} different from that
of Euclidean space. We shall clarify this statement in later sections.

\subsection*{Hyperbolic Neural Networks}

\citet{ganeaHNNs} introduce hyperbolic neural layers which operate on
hyperbolic embeddings and can be arbitrarily stacked into deep networks.  To
save space, we cover this subject in more detail (including references) in the
appendix~\nameref{chap:hyperbolics}. \nameref{chap:manifolds} may be
a prerequisite for that section.

\subsection*{Hyperbolic Image Embeddings}

\citet{khrulkov} endeavour to explore ``hidden hierarchies'' of classes of
objects in image datasets, like \texttt{MiniImageNet}. Specifically, in the
revised pre-print they measure \( \delta \)-hyperbolicity of Euclidean distance
between embeddings of images to reason about ``hierarchicity'' of a dataset.
They then improve off-the-shelf CNNs by appending exponential map layer on top
of them and use hyperbolic distances between embeddings for the final
decision rule.

\subsection{Convolutions for ``hyperbolic arrays''} \label{sec:hconv}

\begin{figure}[ht]\center
\begin{tikzpicture}[scale=0.5, every node/.append style={transform shape}]
\node at (0,0) {
   \input{art/image-convolutions-2019.pdf_tex}
};
\end{tikzpicture}
\caption{A visualization of ``hyperbolic'' CNN for representation learning}
\end{figure}

In \emph{very} short: we (Max Kochurov, Rasoul Karimov, Maria Taktasheva,
Cyrill Mazur, Serge Kozlukov) attempt to construct ``convolutional'' layers
which ``consume'' (multidimensional) arrays of \emph{points on manifolds} (each
point interpreted as a ``descriptor of a pixel'') -- ``hyperbolic arrays''. Our
idea is that hyperbolic space is a continuous analogue of a tree and our
convolutional operation resembles a ``decisison rule for walking down the
tree'', choosing new directions based on descriptors of neighbour pixels. We
also generalize Batch Normalization to such setting be appealing to the notion
of variance and (variance-minimizing) barycenter of a measure in a metric
space. Results are not satisfying, so we do not discuss the model in detail.
The general idea however is relevant to the rest of the work.

\section{Equivariant CNNs} \label{sec:equivariant}

A parallel line of geometric method's development relates to the idea of
equivariance. The most succesful idea in deep learning is arguably that of
convolutional networks. Convolutional neural networks (CNNs) convolve
input signals with a number of learned filters, each filter capturing
its own pattern. To be specific, a convolution of a function \( f: \mathbb{R}^n
\to \mathbb{R} \) and a filter \( h: \mathbb{R}^n \to \mathbb{R} \) is the
function~\cite{feichtingerFAHA}
\[ f*h = x \mapsto \int_{\mathbb{R}^n} f(y + 0) h(x-y)
\operatorname{d\lambda} y. \]
In case of discrete signals such as images this
integral reduces to a finite sum over pixels. An important thing to notice
about this operation, is that it commutes with translations:
\[ (T_a f) * h = f * (T_{-a} h) = T_a (f * h), \]
where \( T_a f = x \mapsto f(x + a) \).

This is exactly what makes the pattern-matching behaviour of CNNs possible: the
filter captures the pattern no matter how the input image has been translated
(i.e. regardless of where the matching object is located). \citet{s2cnn} note
however that such CNNs still have to learn separate filters for rotated
versions of the same object. Further, they consider the problem of spherical
input signals (think heatmaps over the surface of the earth) which possess
rotational and not translational symmetries. However, harmonic analysis can be
performed on groups different from translations, the convolution
formula becoming
\[
(f * h)(x) = \int_G f(g e) h(g^{-1}x) \operatorname{d}g,
\]
for \( f \) and \( h \) of type \( M\to N \), and \( G \) a group acting on
\( N \).

\citet{s2cnn,cohen2018general,e2cnn} build on this idea and propose
convolutional layers equivariant with respect to different groups of
transformations.

For a more detailed treatment of (non-Euclidean) Harmonic analysis one could
refer
to~\citet{axlerHarmonic,
elliott2019generalized,
explorationsHarmonic,
benedettoHarmonic,
stollharmonic,
terrasHarmonicSymmetric,
terrasHarmonicSymmetric2,
fourierS2},
while the bridge to the classic Harmonic analysis could be found in the already
metnioned~\cite{feichtingerFAHA}.
\citet{eyeRotations,zhou2019glosh,scnnNiessner} are another kind of appearances
of Harmonic analysis concepts.

\section{Graph convolutions}

One more direction for geometric methods is neural networks operating on graphs.
We refer the Reader to~\citet{kipf}.

Graph-convolutional methods are also being used for 3D data. The Reader might
take a look at~\citet{edgeconv} which is particularly relevant to this thesis.

\section{Geoopt} \label{sec:geoopt}

This thesis is closely related to the geoopt~\cite{geoopt} package, a joint
effort of Max Kochurov, Rasoul Karimov, and your narrator.
Geoopt is python package for Riemannian optimization in \texttt{Pytorch}.
More background on Riemannian optimization is in the Appendix~\nameref{chap:manifolds}.
Starting this development, the authors hadn't much familiarity with
differential geometry, nor with optimization on manifolds in particular. One of
the consequences is that we simply couldn't have been aware of what use-cases
are important, thus in spite of rather hot discussions, the final API has
significant flaws.

A very short overview of the current design: \texttt{geoopt.Manifold} base
class describes a methodset expected by \texttt{geoopt.optim} optimizers (which
are compatible with \texttt{torch.optim}); this methodset includes
\texttt{retr}action, vector \texttt{transp}ort, \texttt{inner} product, and its
inverse, \texttt{egrad2rgrad}; specific implementations of \texttt{Manifold}
can substittue different versions of e.g. retractions, providing different
levels of precision; points and tangent vectors are always represented by
coordinates in the (assumed) ambient space. In case of \texttt{PoincareBall},
this coincides with the natural global chart and its chart-induced basis vector
fields. Such consistency is only possible because of negative curvature of
Hyperbolic space. On a sphere one could neither allocate a non-vanishing smooth
vector field, nor expect geodesics to exist between all points, nor measures to
have unique barycentres. To be precise, the numbers that Geoopt stores to
represent a tangent vector are actually coordinates of the push-forward of that
vector under assumed embedding into ambient vector space. Restricting ourselves
to such representations wasn't an easy and immediate decision, but at the time
we couldn't find useful use-cases for introducing e.g. local charts.

\subsection{Current limitations of geoopt} \label{sec:geooptTodo}

``...since brevity is soul of wit, and tediousness the limbs and outward
flourishes, I will be brief'':
\begin{itemize}
\item An important use-case for introducing local charts to geoopt is tiling-based
parameterizations of Hyperbolic space, as in~\citet{yaSaTilingBased}
\item As~\citet{trivializations} suggests, we should further our discussion of
``manifold-versus-methodset for optimization'' interface, as in addition to
retraction-based optimization, we may be interested in Riemannian and Lie
exponentials with tangent-space parameterization and adaptive schemes.
\item Currently, dimension of manifold is not part of the \texttt{Manifold}
type nor of \texttt{manifold} object, which limits both type-checking and
inference opportunities.
\item Finally, geoopt is implemented in Pytorch; a functional style rewrite in
JAX would allow less \emph{flawed} design and higher order differentiation, as
well as more consistent language.
\end{itemize}

\section{Statement and methodology} \label{sec:statement}

