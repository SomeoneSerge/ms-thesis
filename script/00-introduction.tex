\chapter{Introduction}

\epigraphhead[50]{%
    \epigraph{"So yeah, thought ya\\
    Might like to go to the show.\\
    I've got some bad news for you, Sunshine -- Pink isn't well!"
    }{Pink Floyd, "The Wall"}
}

In this thesis we\footnote{
Using plural pronouns in single-author manuscripts is a curious tradition that,
aside from all, allows the narrator to sound less pretentios, as if speaking
from outside, avoiding personal claims. The pronoun might however still need
some justification and we propose the following: let's assume that within this
thesis ``we'' refers to narrator and the Reader. This gives meaning to
expressions like ``we consider'', etc.  One more word on pronouns: throughout
this work we treat tokens ``he'' and ``him'' as gender neutral so as to avoid
the ugliness of ``them'', or rather as gender-\emph{less}, so as to promise
that we imply no discrimination}
deal with so called ``geometric methods'' in deep learning having the needs of
inverse graphics in mind. The general idea of geometric deep learning is to
construct slightly less ``un-motivated'' (and, hopefully, more ``efficient'')
architectures by exploiting (possibly ``hidden'') symmetries and curvature in
data. Mainly, we are concerned with so called hyperbolic neural networks,
introduced by Becigneul and Ganea~\cite{ganeaHNNs}, which develop the idea of
learning data representations as points in a hyperbolic (metric) space and
suggest composable neural layers that act directly hyperbolic representations.
Our final goal would be to propose similar neural layers that could be
applicable in computer vision tasks with possible ``hidden hierarchies'', such
as image or pointcloud segmentation, or correspondence synchronization. To
achieve that, we find it fundamental to lie down the diversity of existing
methods and insights in a rigorous and unifying manner. While this may seem a
minor task at first, a closer consideration reveals a number of gaps in the
state-of-the-art publications, as the works in this field rely on concepts from
several independent-yet-interconnected disciplines. For instance, the
unanimously acknowledged and unifying language seems to be that of differential
geometry, which among other things allows formulation of numerical methods for
mathematical optimization in different geometries. When it comes to models of
hyperbolic geometry, a typical differential geometric
textbook~\cite{leeRiem,leeSmooth} tends to skip a thorough discussion of
hyperbolic geometry internals, such as e.g.  inversions, detailed
construction of hyperbolic metric, or exact connections between different
models of hyperbolic geometry. Instead such a textbook would either give a
ready formula for the metric tensor, or better yet leave specific instances of
hyperbolic space as exercises for the reader. Then as main reference serve the
works devoted to geometries specifically~\cite{thurstonThree}.
These, however, often omit set-theoretic formulations working instead on purely
geometric grounds, or use otherwise differing
notation~\cite{beardonGeometryDiscrete}. Such sources provide many insights,
yet it takes an effort to connect these results with the general framework.
Finally, hyperbolic neural networks have borrowed from Physics, notably
including Ungar's gyrovector formalism (which in fact but instantiates Bol
loops~\cite{sabinin1995gyrogroups}).  All three disciplines live their own
lives. Hyperbolic deep learning, taking from all three, suffers from gaps and
incosistencies. Thus we'd hope to (begin to) set a concise and unifying
language for discussion of hyperbolic neural networks with our
appendices~\nameref{appendix:manifolds} and~\nameref{appendix:hyperbolics}.  On
the practical side, we briefly consider image and pointcloud classification
problems, discussing two new proof-of-concept models based on hyperbolic
representations.  Unfortunately, as is these models do not provide desirable
results, so we discuss prospective work that could still make them work.  Much
of this thesis is inspired by the work done in collaboration with Max Kochurov
and Rasoul Karimov, so we also give an overview of those collaborative efforts
in the appendices~(\autoref{appendix:hconv}).
