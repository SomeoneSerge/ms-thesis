\chapter{Introduction}

\epigraphhead[50]{%
    \epigraph{"So yeah, thought ya\\
    Might like to go to the show.\\
    I've got some bad news for you, Sunshine -- Pink isn't well!"
    }{Pink Floyd, "The Wall"}
}

\section*{Foreword}

Until the very last moment, this thesis has been more of an enumeration of
pieces of ideas and experiments, than a single purposeful story. In faith, I've
come to have forgotten what motivated me to choose the subject I chose.  Only
now, the night before submission deadline, I started \emph{remembering}
portions of what I actually hoped to communicate. The common theme across the
thesis the approach of solving problems and discovering things by pointing out
inconsistent \emph{types}\footnote{Actually the onle math I've learned so far
is internalized in category of sets, but it's still illuminating to talk about
types even without defining them}. To see what I mean, consider (well-known)
examples that motivated me: the key to generalizing derivatives from real
functions to functions on arbitrary (even infinite-dimensional) linear spaces
is to note that derivative is in fact an \emph{operator} that acts on
increments in the input; the key to generalizing derivatives to manifolds
without vector structure is to note that the ``increments'' do not come from
original space, but actually are vectors in the tangent space; the key to
generalizing gradient descent algorithm to manifolds is to note that derivative
of an objective function $\mathbb{R}^n \to \mathbb{R}$ is a \emph{row}-vector
i.e. a linear functional that needs to be converted into an increment vector
with transpostion, implicitly applying Riesz representation theorem; the key to
generalizing expontial function to manifolds is to note that in equation
$\dot{x} = x$ the function $x$ is from tangent space to manifold, and that
derivative is in fact the covariant derivative (the linear connection) and not
the pushforward; the key to generalizing convolutions to other domains and
symmetries, is to observe that integation in $\int_{\mathbb{R}^n}
f(y)h(x-y)\operatorname{d}y$ is not over $\operatorname{dom}f=\mathbb{R}^n$,
but over the group $(\mathbb{R}^n,0,+,-)$ of translations acting on
$\operatorname{dom}f$ -- it is not $\operatorname{dom}f$ that matters if we
want to achieve equivariance, it is the group!

\section*{Structure of the thesis}

In this thesis we\footnote{
Using plural pronouns in single-author manuscripts is a curious tradition that,
aside from all, allows the narrator to sound less pretentios, as if speaking
from outside, avoiding personal claims. The pronoun might however still need
some justification and we propose the following: let's assume that within this
thesis ``we'' refers to narrator and the Reader. This gives meaning to
expressions like ``we consider'', etc.  One more word on pronouns: throughout
this work we treat tokens ``he'' and ``him'' as gender neutral so as to avoid
the ugliness of ``them'', or rather as gender-\emph{less}, so as to promise
that we imply no discrimination}
deal with so called ``geometric methods'' in deep learning having the needs of
inverse graphics in mind. The general idea of geometric deep learning is to
construct slightly less ``un-motivated'' (and, hopefully, more ``efficient'')
architectures by exploiting (possibly ``hidden'') symmetries and curvature in
data. Mainly, we are concerned with so called hyperbolic neural networks,
introduced by Becigneul and Ganea~\cite{ganeaHNNs}, which develop the idea of
learning data representations as points in a hyperbolic (metric) space and
suggest composable neural layers that act directly hyperbolic representations.
Our final goal would be to propose similar neural layers that could be
applicable in computer vision tasks with possible ``hidden hierarchies'', such
as image or pointcloud segmentation, or correspondence synchronization. To
achieve that, we find it fundamental to lie down the diversity of existing
methods and insights in a rigorous and unifying manner. While this may seem a
minor task at first, a closer consideration reveals a number of gaps in the
state-of-the-art publications, as the works in this field rely on concepts from
several independent-yet-interconnected disciplines. For instance, the
unanimously acknowledged and unifying language seems to be that of differential
geometry, which among other things allows formulation of numerical methods for
mathematical optimization in different geometries. When it comes to models of
hyperbolic geometry, a typical differential geometric
textbook~\cite{leeRiem,leeSmooth} tends to skip a thorough discussion of
hyperbolic geometry internals, such as e.g.  inversions, detailed
construction of hyperbolic metric, or exact connections between different
models of hyperbolic geometry. Instead such a textbook would either give a
ready formula for the metric tensor, or better yet leave specific instances of
hyperbolic space as exercises for the reader. Then as main reference serve the
works devoted to geometries specifically~\cite{thurstonThree}.
These, however, often omit set-theoretic formulations working instead on purely
geometric grounds, or use otherwise differing
notation~\cite{beardonGeometryDiscrete}. Such sources provide many insights,
yet it takes an effort to connect these results with the general framework.
Finally, hyperbolic neural networks have borrowed from Physics, notably
including Ungar's gyrovector formalism (which in fact but instantiates Bol
loops~\cite{sabinin1995gyrogroups}).  All three disciplines live their own
lives. Hyperbolic deep learning, taking from all three, suffers from gaps and
incosistencies. Thus we'd hope to (begin to) set a concise and unifying
language for discussion of hyperbolic neural networks with our
appendices~\nameref{appendix:manifolds} and~\nameref{appendix:hyperbolics}.  On
the practical side, we briefly consider image and pointcloud classification
problems, discussing two new proof-of-concept models based on hyperbolic
representations.  Unfortunately, as is these models do not provide desirable
results, so we discuss prospective work that could still make them work.  Much
of this thesis is inspired by the work done in collaboration with Max Kochurov
and Rasoul Karimov, so we also give an overview of those collaborative efforts
in the appendices~(\autoref{appendix:hconv}).
