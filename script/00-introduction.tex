\chapter{Introduction}

\epigraphhead[50]{%
    \epigraph{"So yeah, thought ya\\
    Might like to go to the show.\\
    I've got some bad news for you, Sunshine -- Pink isn't well!"
    }{Pink Floyd, "The Wall"}
}

\section*{Informal foreword}

Until the very last moment, this thesis has been more of an enumeration of
pieces of ideas and experiments, than a single purposeful story. In faith, I've
come to have forgotten what motivated me to choose the subject I chose.  Only
now, the night before submission deadline, I started \emph{remembering}
portions of what I actually hoped to communicate. The common theme across the
thesis the approach of solving problems and discovering things by pointing out
the \emph{types}\footnote{Actually the only math I've learned so far is
internalized in the category of sets, but it's still illuminating to talk about
types even without defining them} of objects one operates with. To see what I
mean, consider (well-known) examples that motivated me: they key to Fourier's
breakthroughs~(e.g. introduction to~\cite{explorationsHarmonic}) was to
overcome the paradigm shift of realizing that functions arising as solutions to
his equations aren't described by \emph{formulas}, but by assigning a value to
each point in some domain set, eventually turning the ``intuitive'' notion of a
function into an almost tangible object -- an anything-goes subset of the
Cartesian product of the domain and the range, respecting certain laws --
a ``function'' has an explicit \emph{type}; the key to generalizing derivatives
from real functions to functions on arbitrary (even infinite-dimensional)
linear spaces is to note that derivative is in fact an \emph{operator} that
acts on increments in the input -- the Fr\'echet derivative; the key to
generalizing derivatives to manifolds without vector structure is to note that
the ``increments'' do not come from the original space, but actually belong in the
tangent bundle; the key to generalizing gradient descent algorithm to manifolds
is to note that derivative of an objective function $\mathbb{R}^n \to
\mathbb{R}$ is a \emph{row}-vector i.e. a linear functional that needs to be
converted into an increment vector with transposition, an implicit application
Riesz's representation theorem; an attempt to generalize exponential function
to manifolds points out that in the equation $\dot{x} = x$ the function $x$ is
\emph{actually} from tangent bundle~\cite{nlabExpmap} to the manifold, and that
derivative is, in fact, the covariant derivative (the linear connection) and not
the pushforward; the key to generalizing convolutions to other domains and
symmetries, is to observe that integration in $\int_{\mathbb{R}^n}
f(y)h(x-y)\operatorname{d}y$ is not over $\operatorname{dom}f=\mathbb{R}^n$,
but over the group $(\mathbb{R}^n,0,+,-)$ of translations acting on
$\operatorname{dom}f$ -- it is not $\operatorname{dom}f$ that matters if we
want to achieve equivariance, it is the group!

The work begins with acquaintance with the recently emerged field of hyperbolic deep
learning and very simple idea that in fact in \emph{any} task, not just graphs
and language modeling, the existing models can be underperforming because they
are not accounting for \emph{curvature}, and that perhaps there could be a
generic method of improving models by introducing curvature explicitly.  One
relatively general method that seems to be reliably improving performance is
just fusing a classic ``Euclidean'' neural network with a ``hyperbolic head''
-- formulating distance-based decision rule and measuring hyperbolic distances
between embeddings. This method is limited in that one can only get a constant
``boost'' once and cannot keep further improving performance by introducing
more complexity -- ``stacking more layers''. A naive attempt to construct such
stack-able layers has met a number of obstructions. and here comes the summary
of my work: I'm trying to find a mathematical Framework of \emph{what} we were
implicitly trying to do in our naive implementations. There must be a
\emph{type mismatch} in our naive model and inferring explicit types would
allow to find it and reason about available cures. It is very likely that such
a Framework should be an already existing thing, that I'm simply not familiar
with. Thus I'm trying to find parallels e.g. in Pennec's Riemannian
generalizations of principal component analysis, and in Harmonic analysis
(convolutions).  Later I learn about Cohen and Welling's works on equivariant
models and find many similarities to our problems.

Below is the report on this journey.

\section*{Formal introduction}

In this thesis we\footnote{
Using plural pronouns in single-author manuscripts is a curious tradition that,
aside from all, allows the narrator to sound less pretentios, as if speaking
from outside, avoiding personal claims. The pronoun might however still need
some justification and we propose the following: let's assume that within this
thesis ``we'' refers to narrator and the Reader. This gives meaning to
expressions like ``we consider'', etc.  One more word on pronouns...
I have tried for many years to use "they and
them" as gender-neutral pronouns, yet I shall join~\cite{strictlyEnglish} in
that it is ``an abominable and I want no part in it''.  However, instead of
following the rule of ``masculine be taken to include feminine whenever
necessary'' I would rather strip ``masculine'' of any identity and declare it
the actual gender-less one, all throughout this thesis.}
deal with so-called ``geometric methods'' in deep learning having the needs of
inverse graphics in mind. The general idea of geometric deep learning is to
construct slightly less ``un-motivated'' (and, hopefully, more ``efficient'')
architectures by exploiting (possibly ``hidden'') symmetries and curvature in
data. Mainly, we are concerned with so-called hyperbolic neural networks,
introduced by Becigneul and Ganea~\cite{ganeaHNNs}, which develop the idea of
learning data representations as points in a hyperbolic (metric) space and
suggest composable neural layers that act directly hyperbolic representations.
Our final goal would be to propose similar neural layers that could be
applicable in computer vision tasks with possible ``hidden hierarchies'', such
as image or pointcloud segmentation, or correspondence synchronization. To
achieve that, we find it fundamental to lie down the diversity of existing
methods and insights in a rigorous and unifying manner. While this may seem a
minor task at first, a closer consideration reveals a number of gaps in the
state-of-the-art publications, as the works in this field rely on concepts from
several independent-yet-interconnected disciplines. For instance, the
unanimously acknowledged and unifying language seems to be that of differential
geometry, which among other things allows formulation of numerical methods for
mathematical optimization in different geometries. When it comes to models of
hyperbolic geometry, a typical differential geometric
textbook~\cite{leeRiem,leeSmooth} tends to skip a thorough discussion of
hyperbolic geometry internals, such as e.g. inversions, detailed
construction of hyperbolic metric, or exact connections between different
models of hyperbolic geometry. Instead such a textbook would either give a
ready formula for the metric tensor, or better yet leave specific instances of
hyperbolic space as exercises for the reader. Then as main reference serve the
works devoted to geometries specifically~\cite{thurstonThree}.
These, however, often omit set-theoretic formulations working instead on purely
geometric grounds, or use otherwise differing
notation~\cite{beardonGeometryDiscrete}. Such sources provide many insights,
yet it takes an effort to connect these results with the general framework.
Finally, hyperbolic neural networks have borrowed from Physics, notably
including Ungar's gyrovector formalism (which in fact but instantiates Bol
loops~\cite{sabinin1995gyrogroups}).  All three disciplines live their own
lives. Hyperbolic deep learning, taking from all three, suffers from gaps and
incosistencies. Thus we'd hope to (begin to) set a concise and unifying
language for discussion of hyperbolic neural networks with our
appendices~\nameref{chap:manifolds} and~\nameref{chap:hyperbolics}.  On
the practical side, we briefly consider image and pointcloud classification
problems, discussing two new proof-of-concept models based on hyperbolic
representations.  Unfortunately, as is these models do not provide desirable
results, so we discuss prospective work that could still make them work.  Much
of this thesis is inspired by the work done in collaboration with Max Kochurov
and Rasoul Karimov, so we also give an overview of those collaborative efforts
in ~(\autoref{sec:hconv}).
